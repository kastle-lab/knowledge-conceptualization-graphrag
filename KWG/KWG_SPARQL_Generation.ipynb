{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Q19ZmgtABdY"
      },
      "outputs": [],
      "source": [
        "def load_file_to_string(file_path):\n",
        "  \"\"\"Loads the content of a file into a string.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the file.\n",
        "\n",
        "  Returns:\n",
        "    The content of the file as a string.\n",
        "  \"\"\"\n",
        "  with open(file_path, 'r') as file:\n",
        "    file_contents = file.read()\n",
        "  return file_contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR3ldIqORK_3"
      },
      "outputs": [],
      "source": [
        "# Define the file path and load its content into the 'schema' variable\n",
        "file_path = 'Axiomatization/Complex/Axioms.txt'\n",
        "file_string = load_file_to_string(file_path)\n",
        "schema = file_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uAfRrwJ0Ir1Z"
      },
      "outputs": [],
      "source": [
        "# Define a list of competency questions (CQs)\n",
        "CQs = [\n",
        "    \"What are all the hazard events, their name, start date, end date?\",\n",
        "    \"What are all the places, their names?\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "000Sb0kA51bz"
      },
      "outputs": [],
      "source": [
        "# Define a list of competency questions (CQs)\n",
        "def fill_prompt_template(template_text, values_dict):\n",
        "    for key, value in values_dict.items():\n",
        "        template_text = template_text.replace(f\"{{{key}}}\", value)\n",
        "    return template_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "39Rk-ktQ0rNQ"
      },
      "outputs": [],
      "source": [
        "# Temperature Config for LLM\n",
        "temperature = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6kBIvHfcWELb"
      },
      "outputs": [],
      "source": [
        "# Define the initial system message for the language model\n",
        "initial_system_message = \"\"\"\"\n",
        "You are an expert in knowledge graphs and SPARQL query generation. Your task is to generate SPARQL queries based on provided competency questions and a given schema.\n",
        "\n",
        "Guidelines:\n",
        "Use the schema provided in the context block to determine appropriate classes, properties, and relationships.\n",
        " - Ensure queries follow SPARQL syntax and use prefixes correctly.\n",
        " - Generate queries that efficiently retrieve relevant data while optimizing performance.\n",
        " - If multiple valid queries exist, choose the most concise and efficient one.\n",
        " - Preserve the intent of the competency question while ensuring syntactic correctness.\n",
        " - Give only the SPARQL query and nothing else\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OjBBcfT-WELc"
      },
      "outputs": [],
      "source": [
        "# Define the template prompt for the language model\n",
        "template_prompt = \"\"\"\n",
        "Task:\n",
        "Write a SPARQL query that answers the following competency question:\n",
        "{Insert_CQ_here}\n",
        "\n",
        "Requirements:\n",
        "- Use the schema to determine correct URIs and relationships.\n",
        "- Ensure the query retrieves the necessary information efficiently.\n",
        "- Provide the full SPARQL query without placeholders.\n",
        "\n",
        "Context:\n",
        "Below is the schema of a knowledge graph:\n",
        "{Insert_schema_here}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9fXwNvkRREr"
      },
      "source": [
        "# Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z5XTfqZh40mn"
      },
      "outputs": [],
      "source": [
        "# Set the Mistral API key\n",
        "mistral_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae_oa87iWELb",
        "outputId": "f3d2de8a-617f-487d-8f9c-e3628dd5772a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tUldVY03WELb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for Mistral\n",
        "import os\n",
        "from mistralai import Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E8mxO6rhWELb"
      },
      "outputs": [],
      "source": [
        "# Initialize the Mistral client\n",
        "mistral_client = Mistral(api_key=mistral_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6eHCiyPXWELc"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform inference with Mistral\n",
        "def inference_with_mistral( prompt, model = \"mistral-large-2411\"):\n",
        "  \"\"\"Performs inference with the Mistral language model.\n",
        "\n",
        "  Args:\n",
        "    prompt: The prompt string.\n",
        "    model: The name of the Mistral model to use.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the model's response and the raw response data.\n",
        "  \"\"\"\n",
        "  mistal_messages = []\n",
        "  mistal_messages.append({\"role\": \"system\", \"content\": initial_system_message})\n",
        "  mistral_prompt = prompt\n",
        "  mistal_messages.append({\"role\": \"user\", \"content\": mistral_prompt})\n",
        "  chat_response = mistral_client.chat.complete(\n",
        "    model = model,\n",
        "    messages = mistal_messages,\n",
        "    temperature = temperature\n",
        ")\n",
        "\n",
        "  mistal_messages.append({\"role\": \"assistant\", \"content\": chat_response.choices[0].message.content})\n",
        "  return chat_response.choices[0].message.content, chat_response.model_dump_json(indent = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNrrJBrtDAYZ",
        "outputId": "91405c9e-23c4-4763-a317-1ee447b15d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********\n",
            "What are all the hazard events, their name, start date, end date?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**********\n",
            "What are all the places, their names?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Excel file saved to: cq_mistral_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Iterate through the competency questions and perform inference with Mistral\n",
        "cq_mistral_results = []\n",
        "for cq in CQs:\n",
        "  print(\"*\"*10)\n",
        "  input_data = {\n",
        "        \"Insert_CQ_here\": cq,\n",
        "        \"Insert_schema_here\": schema,\n",
        "    }\n",
        "  filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "  mistral_analysis_result, mistral_analysis_raw = inference_with_mistral(filled_prompt)\n",
        "  time.sleep(1)\n",
        "  cq_mistral_results.append((cq, filled_prompt, mistral_analysis_result, mistral_analysis_raw))\n",
        "  print(cq)\n",
        "  print(\"*\"*10)\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(cq_mistral_results, columns=['CQ', 'Prompt', 'Mistral_Analysis_Result', 'Mistral_Analysis_Raw'])\n",
        "\n",
        "# Save the results to an Excel file\n",
        "excel_file_path = f'{file_path.split(\"/\")[-1]}_cq_mistral_results.xlsx'\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "print(f\"Excel file saved to: {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twp5YUSqRpd3"
      },
      "source": [
        "# Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bnhkJ84tw-kg"
      },
      "outputs": [],
      "source": [
        "# Set the Gemini API key\n",
        "gemini_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Uln2lsTCwIaV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for Gemini\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Initialize the Gemini client\n",
        "gemini_client = genai.Client(api_key=gemini_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8w2VONVXwx7n"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform inference with Gemini\n",
        "def inference_with_gemini(prompt, model = \"gemini-2.0-flash\"):\n",
        "  \"\"\"Performs inference with the Gemini language model.\n",
        "\n",
        "  Args:\n",
        "    prompt: The prompt string.\n",
        "    model: The name of the Gemini model to use.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the model's response and the raw response data.\n",
        "  \"\"\"\n",
        "  gemini_prompt = prompt\n",
        "  response = gemini_client.models.generate_content(\n",
        "      model=model,\n",
        "      config=types.GenerateContentConfig(\n",
        "          system_instruction=initial_system_message,\n",
        "          temperature=temperature\n",
        "          ),\n",
        "      contents=gemini_prompt\n",
        "      )\n",
        "  return response.text, response.model_dump_json(indent = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dJPpou4xq5n",
        "outputId": "36119963-37ca-4917-faf7-139b3f874116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********\n",
            "What are all the hazard events, their name, start date, end date?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**********\n",
            "What are all the places, their names?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Excel file saved to: cq_Gemini_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Iterate through the competency questions and perform inference with Gemini\n",
        "cq_gemini_results = []\n",
        "for cq in CQs:\n",
        "  print(\"*\"*10)\n",
        "  input_data = {\n",
        "        \"Insert_CQ_here\": cq,\n",
        "        \"Insert_schema_here\": schema,\n",
        "    }\n",
        "  filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "  gemini_analysis_result, gemini_analysis_raw = inference_with_gemini(filled_prompt)\n",
        "  time.sleep(1)\n",
        "  cq_gemini_results.append((cq, filled_prompt, gemini_analysis_result, gemini_analysis_raw))\n",
        "  print(cq)\n",
        "  print(\"*\"*10)\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(cq_gemini_results, columns=['CQ', 'Prompt', 'Gemini_Analysis_Result', 'Gemini_Analysis_Raw'])\n",
        "\n",
        "# Save the results to an Excel file\n",
        "excel_file_path = f'{file_path.split(\"/\")[-1]}_cq_Gemini_results.xlsx'\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "print(f\"Excel file saved to: {excel_file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
