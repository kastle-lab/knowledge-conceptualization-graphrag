{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Q19ZmgtABdY"
      },
      "outputs": [],
      "source": [
        "def load_file_to_string(file_path):\n",
        "  \"\"\"Loads the content of a file into a string.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the file.\n",
        "\n",
        "  Returns:\n",
        "    The content of the file as a string.\n",
        "  \"\"\"\n",
        "  with open(file_path, 'r') as file:\n",
        "    file_contents = file.read()\n",
        "  return file_contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BR3ldIqORK_3"
      },
      "outputs": [],
      "source": [
        "# Define the file path and load its content into the 'schema' variable\n",
        "file_path = '../KWG/Complex/Axiomatization/Axioms.txt'\n",
        "file_string = load_file_to_string(file_path)\n",
        "schema = file_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uAfRrwJ0Ir1Z"
      },
      "outputs": [],
      "source": [
        "# Define a list of competency questions (CQs)\n",
        "CQs = [\n",
        "    \"What are all the hazard events, their name, start date, end date?\",\n",
        "    \"What are all the places, their names?\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "000Sb0kA51bz"
      },
      "outputs": [],
      "source": [
        "# Define a list of competency questions (CQs)\n",
        "def fill_prompt_template(template_text, values_dict):\n",
        "    for key, value in values_dict.items():\n",
        "        template_text = template_text.replace(f\"{{{key}}}\", value)\n",
        "    return template_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "39Rk-ktQ0rNQ"
      },
      "outputs": [],
      "source": [
        "# Temperature Config for LLM\n",
        "temperature = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6kBIvHfcWELb"
      },
      "outputs": [],
      "source": [
        "# Define the initial system message for the language model\n",
        "initial_system_message = \"\"\"\"\n",
        "You are an expert in knowledge graphs and SPARQL query generation. Your task is to generate SPARQL queries based on provided competency questions and a given schema.\n",
        "\n",
        "Guidelines:\n",
        "Use the schema provided in the context block to determine appropriate classes, properties, and relationships.\n",
        " - Ensure queries follow SPARQL syntax and use prefixes correctly.\n",
        " - Generate queries that efficiently retrieve relevant data while optimizing performance.\n",
        " - If multiple valid queries exist, choose the most concise and efficient one.\n",
        " - Preserve the intent of the competency question while ensuring syntactic correctness.\n",
        " - Give only the SPARQL query and nothing else\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OjBBcfT-WELc"
      },
      "outputs": [],
      "source": [
        "# Define the template prompt for the language model\n",
        "template_prompt = \"\"\"\n",
        "Task:\n",
        "Write a SPARQL query that answers the following competency question:\n",
        "{Insert_CQ_here}\n",
        "\n",
        "Requirements:\n",
        "- Use the schema to determine correct URIs and relationships.\n",
        "- Ensure the query retrieves the necessary information efficiently.\n",
        "- Provide the full SPARQL query without placeholders.\n",
        "\n",
        "Context:\n",
        "Below is the schema of a knowledge graph:\n",
        "{Insert_schema_here}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9fXwNvkRREr"
      },
      "source": [
        "# Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z5XTfqZh40mn"
      },
      "outputs": [],
      "source": [
        "# Set the Mistral API key\n",
        "mistral_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae_oa87iWELb",
        "outputId": "f3d2de8a-617f-487d-8f9c-e3628dd5772a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tUldVY03WELb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for Mistral\n",
        "import os\n",
        "from mistralai import Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E8mxO6rhWELb"
      },
      "outputs": [],
      "source": [
        "# Initialize the Mistral client\n",
        "mistral_client = Mistral(api_key=mistral_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6eHCiyPXWELc"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform inference with Mistral\n",
        "def inference_with_mistral( prompt, model = \"mistral-large-2411\"):\n",
        "  \"\"\"Performs inference with the Mistral language model.\n",
        "\n",
        "  Args:\n",
        "    prompt: The prompt string.\n",
        "    model: The name of the Mistral model to use.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the model's response and the raw response data.\n",
        "  \"\"\"\n",
        "  mistal_messages = []\n",
        "  mistal_messages.append({\"role\": \"system\", \"content\": initial_system_message})\n",
        "  mistral_prompt = prompt\n",
        "  mistal_messages.append({\"role\": \"user\", \"content\": mistral_prompt})\n",
        "  chat_response = mistral_client.chat.complete(\n",
        "    model = model,\n",
        "    messages = mistal_messages,\n",
        "    temperature = temperature\n",
        ")\n",
        "\n",
        "  mistal_messages.append({\"role\": \"assistant\", \"content\": chat_response.choices[0].message.content})\n",
        "  return chat_response.choices[0].message.content, chat_response.model_dump_json(indent = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNrrJBrtDAYZ",
        "outputId": "91405c9e-23c4-4763-a317-1ee447b15d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********\n",
            "What are all the hazard events, their name, start date, end date?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**********\n",
            "What are all the places, their names?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Excel file saved to: cq_mistral_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Iterate through the competency questions and perform inference with Mistral\n",
        "cq_mistral_results = []\n",
        "for cq in CQs:\n",
        "  print(\"*\"*10)\n",
        "  input_data = {\n",
        "        \"Insert_CQ_here\": cq,\n",
        "        \"Insert_schema_here\": schema,\n",
        "    }\n",
        "  filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "  mistral_analysis_result, mistral_analysis_raw = inference_with_mistral(filled_prompt)\n",
        "  time.sleep(1)\n",
        "  cq_mistral_results.append((cq, filled_prompt, mistral_analysis_result, mistral_analysis_raw))\n",
        "  print(cq)\n",
        "  print(\"*\"*10)\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(cq_mistral_results, columns=['CQ', 'Prompt', 'Mistral_Analysis_Result', 'Mistral_Analysis_Raw'])\n",
        "\n",
        "# Save the results to an Excel file\n",
        "excel_file_path = f'{file_path.split(\"/\")[-1]}_cq_mistral_results.xlsx'\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "print(f\"Excel file saved to: {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twp5YUSqRpd3"
      },
      "source": [
        "# Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bnhkJ84tw-kg"
      },
      "outputs": [],
      "source": [
        "# Set the Gemini API key\n",
        "gemini_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Uln2lsTCwIaV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for Gemini\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Initialize the Gemini client\n",
        "gemini_client = genai.Client(api_key=gemini_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8w2VONVXwx7n"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform inference with Gemini\n",
        "def inference_with_gemini(prompt, model = \"gemini-2.0-flash\"):\n",
        "  \"\"\"Performs inference with the Gemini language model.\n",
        "\n",
        "  Args:\n",
        "    prompt: The prompt string.\n",
        "    model: The name of the Gemini model to use.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the model's response and the raw response data.\n",
        "  \"\"\"\n",
        "  gemini_prompt = prompt\n",
        "  response = gemini_client.models.generate_content(\n",
        "      model=model,\n",
        "      config=types.GenerateContentConfig(\n",
        "          system_instruction=initial_system_message,\n",
        "          temperature=temperature\n",
        "          ),\n",
        "      contents=gemini_prompt\n",
        "      )\n",
        "  return response.text, response.model_dump_json(indent = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dJPpou4xq5n",
        "outputId": "36119963-37ca-4917-faf7-139b3f874116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********\n",
            "What are all the hazard events, their name, start date, end date?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**********\n",
            "What are all the places, their names?\n",
            "**********\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Excel file saved to: cq_Gemini_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Iterate through the competency questions and perform inference with Gemini\n",
        "cq_gemini_results = []\n",
        "for cq in CQs:\n",
        "  print(\"*\"*10)\n",
        "  input_data = {\n",
        "        \"Insert_CQ_here\": cq,\n",
        "        \"Insert_schema_here\": schema,\n",
        "    }\n",
        "  filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "  gemini_analysis_result, gemini_analysis_raw = inference_with_gemini(filled_prompt)\n",
        "  time.sleep(1)\n",
        "  cq_gemini_results.append((cq, filled_prompt, gemini_analysis_result, gemini_analysis_raw))\n",
        "  print(cq)\n",
        "  print(\"*\"*10)\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "df = pd.DataFrame(cq_gemini_results, columns=['CQ', 'Prompt', 'Gemini_Analysis_Result', 'Gemini_Analysis_Raw'])\n",
        "\n",
        "# Save the results to an Excel file\n",
        "excel_file_path = f'{file_path.split(\"/\")[-1]}_cq_Gemini_results.xlsx'\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "print(f\"Excel file saved to: {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.70.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.71.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Downloading openai-1.71.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.0/599.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.70.0\n",
            "    Uninstalling openai-1.70.0:\n",
            "      Successfully uninstalled openai-1.70.0\n",
            "Successfully installed openai-1.71.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenAI\n",
        "openai.api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Prepare batch input\n",
        "batch_input_path = f\"{file_path.split(\"/\")[-1]}_gpt4o_batch_input.jsonl\"\n",
        "with open(batch_input_path, \"w\") as f:\n",
        "    for i, cq in enumerate(CQs):\n",
        "        custom_id = f\"cq-{i}\"\n",
        "        input_data = {\n",
        "            \"Insert_CQ_here\": cq,\n",
        "            \"Insert_schema_here\": schema\n",
        "        }\n",
        "        filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "        prompt_payload = {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"gpt-4o\",\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": initial_system_message},\n",
        "                    {\"role\": \"user\", \"content\": filled_prompt}\n",
        "                ],\n",
        "                \"temperature\": temperature,\n",
        "            }\n",
        "        }\n",
        "        f.write(json.dumps(prompt_payload) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Upload file\n",
        "file_upload = openai.files.create(file=open(batch_input_path, \"rb\"), purpose=\"batch\")\n",
        "file_id = file_upload.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch started with ID: batch_67f53335b0d48190b43c143094e182b3\n"
          ]
        }
      ],
      "source": [
        "# 3. Create batch\n",
        "batch = openai.batches.create(\n",
        "    input_file_id=file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\"\n",
        ")\n",
        "batch_id = batch.id\n",
        "print(f\"Batch started with ID: {batch_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: in_progress\n",
            "Status: completed\n",
            "Batch completed with status: completed\n"
          ]
        }
      ],
      "source": [
        "# 4. Poll until done\n",
        "while True:\n",
        "    current = openai.batches.retrieve(batch_id)\n",
        "    print(f\"Status: {current.status}\")\n",
        "    if current.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        print(f\"Batch completed with status: {current.status}\")\n",
        "        break\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Download results\n",
        "output_file_id = current.output_file_id\n",
        "if output_file_id is None:\n",
        "    raise ValueError(\"No output file available.\")\n",
        "\n",
        "output_file = openai.files.content(output_file_id)\n",
        "batch_output = output_file.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Parse and match results\n",
        "results_dict = {}\n",
        "for line in batch_output.strip().split(\"\\n\"):\n",
        "    data = json.loads(line)\n",
        "    custom_id = data[\"custom_id\"]\n",
        "    response_text = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "    results_dict[custom_id] = {\n",
        "        \"result\": response_text,\n",
        "        \"raw\": json.dumps(data, indent=4)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Map results back to original CQs\n",
        "cq_gpt4o_results = []\n",
        "for i, cq in enumerate(CQs):\n",
        "    custom_id = f\"cq-{i}\"\n",
        "    input_data = {\n",
        "        \"Insert_CQ_here\": cq,\n",
        "        \"Insert_schema_here\": schema\n",
        "    }\n",
        "    filled_prompt = fill_prompt_template(template_prompt, input_data)\n",
        "    res = results_dict.get(custom_id, {\"result\": \"\", \"raw\": \"{}\"})\n",
        "    cq_gpt4o_results.append((cq, filled_prompt, res[\"result\"], res[\"raw\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excel file saved to: Axioms.txt_cq_GPT4o_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "# 8. Save to Excel\n",
        "df = pd.DataFrame(cq_gpt4o_results, columns=[\"CQ\", \"Prompt\", \"GPT4o_Result\", \"GPT4o_Raw\"])\n",
        "excel_file_path = f\"{file_path.split('/')[-1]}_cq_GPT4o_results.xlsx\"\n",
        "df.to_excel(excel_file_path, index=False)\n",
        "print(f\"Excel file saved to: {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
